####Load in Resting State time series data 
#find Resting State time series files 
files_RS_resid_ts <- list.files(path=".", recursive = T, full.names = F, pattern="^sub.*_RS_2mm_GSR_glasser_tian_meants\\.csv$")

#confirm that csvs aren't empty 
files_RS_resid_ts[file.size(files_RS_resid_ts)==0]

#create list of IDS
ptlist <-paste("SPN01", substring(files_RS_resid_ts, 5, 7), substring(files_RS_resid_ts, 8, 11), sep = "_")

#read in time series files 
resid_ts <- lapply(files_RS_resid_ts, read.csv, header= F)

#transpose dfs (swapping the rows with the columns and the columns with the row, to put in the correct format to be in a matrix)
resid_ts <-lapply(resid_ts, t)

#Name dfs with participant IDS
names(resid_ts) <-ptlist 

install.packages("rdist")
library(rdist)

#### Calculating Pearson's correaltion on all participant time series files stored in resid_ts
#Calculate the correlation 
for (x in names(resid_ts)) {
  corr= resid_ts[[x]] <- cor(resid_ts[[x]], method = "pearson")
} 

#Next, must preform a Fisher transformation on the matrices 
install.packages("DescTools") 
library(DescTools)

for (p in names(resid_ts)){
  zt = resid_ts[[p]] <- FisherZ(resid_ts[[p]])
  resid_ts[[p]][is.infinite(resid_ts[[p]])] <-NA
}

#Extracting the lower triangle of the matrix and vectorizing
for (p in names(resid_ts)){
  lower.tri(resid_ts[[p]], diag=FALSE)
  resid_ts[[p]][lower.tri(resid_ts[[p]])] <-NA 
  vt = resid_ts[[p]] <-as.vector(t(resid_ts[[p]]))
  vt1 = resid_ts[[p]] <- resid_ts[[p]][!is.na(resid_ts[[p]])]
  
}

#Get all participant vectors into one dataframe 
v_resid_ts <- resid_ts
matrixf<- as.data.frame(do.call(rbind, v_resid_ts))

#### Running neuroComBat to account for scanner effects 
##Install that package 
install.packages("devtools")
library(devtools)
install_github("jfortin1/neuroCombatData")
install_github("jfortin1/neuroCombat_Rpackage")

# run ComBat on connectivity data to harmonize across scanners
library(neuroCombat)

#Must transpose so that the participants are in the columns and the correlation values are the rows 
matrixf_2 <- as.matrix(matrixf) #first convert to a matrix, and then transpose 
matrixf_1 <- t(matrixf_2)

# dat is a data matrix of the data to harmonize - rows are features (connections) and columns are participants
rs_corrs_com <- matrixf_1

###Import data to be used as design matrix 
add_data <- read.csv("/mnt/tigrlab/projects/tsecara/QCandInclusion/SPINS_finalregression_RS_reorderd.csv")
add_data$diagnostic_group <- as.factor(add_data$diagnostic_group)
add_data$scanner <- as.factor(add_data$scanner)

# mod is a design matrix specifying biological covariates that should be protected - here diagnosis, age, sex, and cog variables
modcombat <- model.matrix(~diagnostic_group +sex + age +
                            avg_fd_RS + scog_mean_ea + simulation + mentalizing + wtar_std_score + scog_iri_total + np_domain_tscore_process_speed +
                            np_domain_tscore_att_vigilance + np_domain_tscore_work_mem +np_domain_tscore_verbal_learning + np_domain_tscore_visual_learning
                          + np_domain_tscore_reasoning_ps + np_domain_tscore_social_cog + bsfs_sec1_total + bsfs_sec2_total + bsfs_sec3_total +
                            bsfs_sec4_total + bsfs_sec5_total + bsfs_sec6_total, data=add_data)

# R run ComBat
# batch is a vector (length should be equal to the number of columns in the data matrix) that specifies the id for the batch, site, or scanner to correct for
rs_combat <- neuroCombat(dat=rs_corrs_com, batch=c(add_data$scanner), mod=modcombat)


# transpose the harmonized data matrix
rs_combat_data <- t(rs_combat$dat.combat)

# identify top 10% most variable connections as measured by median absolute deviation to reduce number of connectivity inputs (more robust against outliers than standard deviation as per Xia et al., 2018)
mad_conn <- apply(rs_combat_data, 2, mad)
# 76636*.1 = 7664
top_10 <- names(sort(mad_conn, decreasing=TRUE)[1:7664])
# create separate X and Y matrices
conn_top_10 <- colnames(rs_combat_data[,colnames(rs_combat_data) %in% top_10])
conn <- as.matrix(rs_combat_data[,colnames(rs_combat_data) %in% top_10]) # x var

##### SPLITTING SAMPLE BASED ON NEURCOGNTIVE PERFORMANCE 
#Load in CSV containing participant data, specifically MCCB Reasoning and Problem Solving Score 
data <- read.csv("/mnt/tigrlab/projects/tsecara/QCandInclusion/SPINS_final_regressiondata_UPDATED_r_RS.csv")

#Split sample into three based on MCCB Reasoning performance 
mental_sort = data[order(data$np_domain_tscore_reasoning_ps),]
LowDF <- mental_sort[1:115,]
HighDF <- mental_sort[232:346,]

#Combine into one dataframe containing participants with good and poor cognitive performers 
combined_df <- rbind(LowDF, HighDF)

#Exact the names for the high and low performing group 
highnames <- HighDF$record_id
lownames <- LowDF$record_id
all_names <- combined_df$record_id

#Next, going to select and isolate the connectivity values only for the high and low performers
conn_total <- conn[c(all_names),]

#Next, going to order by high and low to calculate the standard deviation  
conn_high <- conn[c(highnames),]
conn_low <- conn[c(lownames),]

conn_high_sd <- as.matrix(t(apply(conn_high, 2, sd))) #finding the standard deviation of the columns for good neurocogntive performers
conn_low_sd <- as.matrix(t(apply(conn_low, 2, sd))) #repeating for poor neurocogntive performers 

###FORMATTING DATA IN PREP FOR PERMUTATION ANALYSIS 
conn_high_sd_n <- colnames(conn_high_sd)
new_conn2 = matrix(0, 1, 76636)
thisone <- as.matrix(t(rs_combat_data[1,]))
dimnames(new_conn2) = dimnames(thisone)
new_conn2[,conn_high_sd_n] = conn_high_sd

conn_low_sd_n <- colnames(conn_low_sd)
new_conn2_low = matrix(0, 1, 76636)
thisone <- as.matrix(t(rs_combat_data[1,]))
dimnames(new_conn2_low) = dimnames(thisone)
new_conn2_low[,conn_low_sd_n] = conn_low_sd

f <- function(b) {
  b[lower.tri(b)] <- t(b)[lower.tri(b)]
  b
}

#try for the first participant, placing the values into a matrix form once again 
a= new_conn2
b1= matrix(0,392,392)
b1[lower.tri(b1, diag=FALSE)] <- a
b <- t(b1)
b

new_mat_avg_high <- f(b)

a= new_conn2_low
b1= matrix(0,392,392)
b1[lower.tri(b1, diag=FALSE)] <- a
b <- t(b1)
b

new_mat_avg_low <- f(b)

G_atlas_info_reordered <- read.csv("/mnt/tigrlab/projects/tsecara/G_atlas_info_reorder_2.csv")
rnames <- c(1:392)
rownames(new_mat_avg_high) = rnames
colnames(new_mat_avg_high) = rnames
colnames(new_mat_avg_high) = rownames(new_mat_avg_high)
new_mat_avg2_high <-as.matrix(new_mat_avg_high)

new_mat_avg3_high <-new_mat_avg2_high[(G_atlas_info_reordered$index),(G_atlas_info_reordered$index)]

rnames <- c(1:392)
rownames(new_mat_avg_low) = rnames
colnames(new_mat_avg_low) = rnames
colnames(new_mat_avg_low) = rownames(new_mat_avg_low)
new_mat_avg2_low <-as.matrix(new_mat_avg_low)

new_mat_avg3_low <-new_mat_avg2_low[(G_atlas_info_reordered$index),(G_atlas_info_reordered$index)]

highment_vis <- new_mat_avg3_high[351:354, 267:343] #selecting for the specific network - change this to get values 
highment_vis[highment_vis==0]<-NA #Need to set the all the zeros to NA so that they are not considered 
lowment_vis <- new_mat_avg3_low[351:354, 267:343]
lowment_vis[lowment_vis==0]<-NA

#going to select for the upper triangle and vectorize each matrix 
lower.tri(highment_vis, diag=FALSE)
highment_vis[lower.tri(highment_vis)]
highment_mat <-as.vector(t(highment_vis))
highment_mat <- highment_mat[!is.na(highment_mat)]
highment_mat_f <- t(as.data.frame(highment_mat))
highment_value <- rowMeans(highment_mat_f) #Averaging the connectivity and need to put this into the colum of perm table

lower.tri(lowment_vis, diag=FALSE) #Again for the poor performing group 
lowment_vis[lower.tri(lowment_vis)]
lowment_mat <-as.vector(t(lowment_vis))
lowment_mat <- lowment_mat[!is.na(lowment_mat)]
lowment_mat_f <- t(as.data.frame(lowment_mat))
lowment_value <- rowMeans(lowment_mat_f) 

test.stat1 <- highment_value - lowment_value
test.stat1

##### PERMUTATION ANALYSIS BEINGS HERE
#Hypothetically the permutation begins here
set.seed(1314)

# the number of permutation samples to take
P <- 1000

#Loading in the function required for this f-loop
#Convert everyone into a matrix
f <- function(b) {
  b[lower.tri(b)] <- t(b)[lower.tri(b)]
  b
}

#Loding the csv required for f-loop
G_atlas_info_reordered <- read.csv("/mnt/tigrlab/projects/tsecara/G_atlas_info_reorder_2.csv")

#going to create a numeric strings to put values in, which will be combined into a data frame afterwords 
result_group1 <- numeric(P)
result_group2 <- numeric(P)

# Split Data into Training and Testing in R 
sample_size = floor(0.5*nrow(conn_total)) #denoting that we want to split the data in half each time 

for(i in 1:P){
  # randomly split data in r
  picked = sample(seq_len(nrow(conn_total)),size = sample_size, replace=FALSE)
  group_1 =conn_total[picked,]
  group_2 =conn_total[-picked,]
  group_1_sd <- as.matrix(t(apply(group_1, 2, sd))) #finding the standard deviation of the columns 
  group_2_sd <- as.matrix(t(apply(group_2, 2, sd)))
  
  group_1_sd_n <- colnames(group_1_sd)
  new_conn = matrix(0, 1, 76636)
  thisone <- as.matrix(t(ea_combat_data[1,]))
  dimnames(new_conn) = dimnames(thisone)
  new_conn[,group_1_sd_n] = group_1_sd
  
  group_2_sd_n <- colnames(group_2_sd)
  new_conn2 = matrix(0, 1, 76636)
  thisone2 <- as.matrix(t(ea_combat_data[1,]))
  dimnames(new_conn2) = dimnames(thisone2)
  new_conn2[,group_2_sd_n] = group_2_sd
  
  #try for the first participant, placing the values into a matrix form once again 
  a= new_conn
  b1= matrix(0,392,392)
  b1[lower.tri(b1, diag=FALSE)] <- a
  b <- t(b1)
  b
  group_1_avgmat <- f(b)
  diag(group_1_avgmat)= 0
  
  a= new_conn2
  b1= matrix(0,392,392)
  b1[lower.tri(b1, diag=FALSE)] <- a
  b <- t(b1)
  b
  group_2_avgmat <- f(b)
  diag(group_2_avgmat)= 0
  
  
  rnames <- c(1:392)
  rownames(group_1_avgmat) = rnames
  colnames(group_1_avgmat) = rnames
  colnames(group_1_avgmat) = rownames(group_1_avgmat)
  group_1_avgmat2 <-as.matrix(group_1_avgmat)
  group_1_avgmat3 <-group_1_avgmat2[(G_atlas_info_reordered$index),(G_atlas_info_reordered$index)]
  
  rnames <- c(1:392)
  rownames(group_2_avgmat) = rnames
  colnames(group_2_avgmat) = rnames
  colnames(group_2_avgmat) = rownames(group_2_avgmat)
  group_2_avgmat2 <-as.matrix(group_2_avgmat)
  group_2_avgmat3 <-group_2_avgmat2[(G_atlas_info_reordered$index),(G_atlas_info_reordered$index)]
  
  group_1_vis <- group_1_avgmat3[351:354, 267:343]
  group_1_vis[group_1_vis==0]<-NA #Need to set the all the zeros to NA so that they are not considered 
  group_2_vis <- group_2_avgmat3[351:354, 267:343]
  group_2_vis[group_2_vis==0]<-NA
  
  #going to select for the upper triangle and vectorize each matrix 
  lower.tri(group_1_vis, diag=FALSE)
  group_1_vis[lower.tri(group_1_vis)]
  group_1mat <-as.vector(t(group_1_vis))
  group_1mat <- group_1mat[!is.na(group_1mat)]
  group1_mat_f <- t(as.data.frame(group_1mat))
  result_group1[i] <- rowMeans(group1_mat_f) #Averaging the connectivity #Need to put this into the colum of perm table
  
  lower.tri(group_2_vis, diag=FALSE)
  group_2_vis[lower.tri(group_2_vis)]
  group_2mat <-as.vector(t(group_2_vis))
  group_2mat <- group_2mat[!is.na(group_2mat)]
  group2_mat_f <- t(as.data.frame(group_2mat))
  result_group2[i] <- rowMeans(group2_mat_f) #Averaging the connectivity 
}
vis_network_perm <- data.frame(rbind(result_group1, result_group2))

rownames(vis_network_perm) <- c("groupone", "grouptwo")
vis_network_perm <- data.frame(t(vis_network_perm))
vis_network_perm$diff <- (vis_network_perm$groupone - vis_network_perm$grouptwo)

library(tidyverse)
reordered <- vis_network_perm %>% arrange(desc(diff))
rown <- c(1:1000)
rownames(reordered) = rown

mean_val_DMN_VMN= mean(reordered$diff <= test.stat1)

#List of extractions for network selection
#new_conn_list_vis[[x]] <- new_conn_list2[[x]][1:60, 1:60]
#new_conn_list_somat[[x]] <- new_conn_list2[[x]][61:99, 61:99]
#new_conn_list_aud[[x]] <- new_conn_list2[[x]][100:114, 100:114]
#new_conn_list_salence[[x]] <- new_conn_list2[[x]][115:170, 115:170]
#new_conn_list_DAN[[x]] <- new_conn_list2[[x]][171:193, 171:193]
#new_conn_list_lang[[x]] <- new_conn_list2[[x]][194:216, 194:216]
#new_conn_list_FPN[[x]] <- new_conn_list2[[x]][217:266, 217:266]
#new_conn_list_default[[x]] <- new_conn_list2[[x]][267:343, 267:343]
#new_conn_list_PMN[[x]] <- new_conn_list2[[x]][344:350, 344:350]
#new_conn_list_VMN[[x]] <- new_conn_list2[[x]][351:354, 351:354]
#new_conn_list_OAN[[x]] <- new_conn_list2[[x]][355:360, 355:360]
# for subcortex its 360:392, 360:392

